MULTILAYER PERCEPTRON

import numpy as np

# Define the activation functions and their derivatives
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
    return x * (1 - x)

def tanh(x):
    return np.tanh(x)

def tanh_derivative(x):
    return 1 - x**2

def relu(x):
    return np.maximum(0, x)

def relu_derivative(x):
    return 1. * (x > 0)

# Get user input for the number of inputs, hidden layers, neurons, and activation function
num_inputs = int(input("Enter the number of inputs: "))
num_hidden_layers = int(input("Enter the number of hidden layers: "))
num_neurons = int(input("Enter the number of neurons in the hidden layer: "))
activation_function = input("Enter the activation function (sigmoid, tanh, or relu): ").lower()

# Function to select the appropriate activation function and its derivative
def select_activation_function(name):
    if name == "sigmoid":
        return sigmoid, sigmoid_derivative
    elif name == "tanh":
        return tanh, tanh_derivative
    elif name == "relu":
        return relu, relu_derivative
    else:
        raise ValueError("Invalid activation function name")

# Create random initial weights for the input and hidden layers
hidden_layer_weights = 2 * np.random.random((num_inputs, num_neurons)) - 1
output_layer_weights = 2 * np.random.random((num_neurons, 1)) - 1

# Training parameters
learning_rate = 0.1
num_epochs = 10000

# Training data (you can replace this with your own dataset)
X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
y = np.array([[0], [1], [1], [0]])

# Select the activation function and its derivative based on user input
activation_func, activation_derivative = select_activation_function(activation_function)

# Training the MLP with backpropagation
for epoch in range(num_epochs):
    # Forward propagation
    hidden_layer_input = np.dot(X, hidden_layer_weights)
    hidden_layer_output = activation_func(hidden_layer_input)
    output_layer_input = np.dot(hidden_layer_output, output_layer_weights)
    output_layer_output = sigmoid(output_layer_input)  # sigmoid because of binary classification

    # Calculate the error
    error = y - output_layer_output

    # Backpropagation
    d_output = error * sigmoid_derivative(output_layer_output)
    error_hidden_layer = d_output.dot(output_layer_weights.T)
    d_hidden_layer = error_hidden_layer * activation_derivative(hidden_layer_output)

    # Update weights
    output_layer_weights += hidden_layer_output.T.dot(d_output) * learning_rate
    hidden_layer_weights += X.T.dot(d_hidden_layer) * learning_rate

# Print the final output
print("Output after training:")
print(output_layer_output)


OUTPUT:

Enter the number of inputs: 2
Enter the number of hidden layers: 1
Enter the number of neurons in the hidden layer: 2
Enter the activation function (sigmoid, tanh, or relu): sigmoid
Output after training:
[[0.55056778]
 [0.4893086 ]
 [0.53764442]
 [0.40140254]]


